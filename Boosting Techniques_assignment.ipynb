{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858edc03-f561-4fc1-805c-3a594ed607dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Boosting Techniques assignment##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd3616-cc7a-45af-9637-dbaa005b438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u001d",
    "#1.What is Boosting in Machine Learning\f",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong predictive model. \n",
    "It works sequentially, where each new model focuses on correcting the errors of the previous one, gradually improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a9874-be1c-4ab9-94c8-37fd0d36db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.How does Boosting differ from Bagging\f",
    "\n",
    "\u001bBoosting builds models sequentially, with each new model focusing on the mistakes of the previous ones.\n",
    "Bagging (e.g., Random Forest) trains models independently in parallel and combines them by averaging (for regression) or voting (for classification).\n",
    "Boosting reduces bias, while bagging primarily reduces variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cad285-db36-447f-8141-3ae85658b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. What is the key idea behind AdaBoost\f",
    "\n",
    "AdaBoost (Adaptive Boosting) assigns higher weights to misclassified samples, \n",
    "forcing subsequent models to focus on difficult cases. It iteratively adjusts these weights to minimize errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed8633-3cf7-43b5-a923-be8d396152ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Explain the working of AdaBoost with an example4\n",
    "1.\u0007Start with equal weights for all training samples.\n",
    "2.Train a weak model (e.g., a shallow decision tree).\n",
    "3.Increase weights of misclassified samples so the next model focuses on them.\n",
    "4.Repeat this process for multiple iterations.\n",
    "5.Final prediction is a weighted combination of all models.\n",
    "Example: Suppose we classify emails as spam/non-spam. If the first weak model misclassifies certain spam emails, \n",
    "the next model will give those emails more importance, improving accuracy over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff819838-ad4d-42ac-b868-0d77b893dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.What is Gradient Boosting, and how is it different from AdaBoost\f",
    "\n",
    "\u0005Gradient Boosting minimizes the loss function by fitting new models to the residual errors of previous models, optimizing through gradient descent.\n",
    "AdaBoost adjusts weights for misclassified samples, while Gradient Boosting directly optimizes errors using gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2ab5f-00a0-46fd-a261-391b31363063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.What is the loss function in Gradient Boosting\f",
    "\n",
    "The loss function depends on the problem type:\n",
    "\n",
    "Regression: Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n",
    "Classification: Log loss (for binary classification) or Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110dbf8-c062-48c8-977e-745fbb019b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.How does XGBoost improve over traditional Gradient Boosting\f",
    "\n",
    "XGBoost (Extreme Gradient Boosting) enhances traditional Gradient Boosting by:\n",
    "\n",
    "Regularization (L1 & L2) to prevent overfitting.\n",
    "Tree Pruning for efficient learning.\n",
    "Parallel Processing for faster training.\n",
    "Handling missing values automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e7b2d-789c-4a84-8248-d2ec5c2f05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.What is the difference between XGBoost and CatBoost\f",
    "\n",
    "\u0018XGBoost: Optimized for speed and efficiency, supports handling missing values but requires encoding categorical data.\n",
    "CatBoost: Specifically designed for categorical data, uses ordered boosting to prevent target leakage and handles categorical features natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d806606-c08d-4432-bbea-4d051b39d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.What are some real-world applications of Boosting techniques\f",
    "\n",
    "\u001d",
    "\u000eFraud Detection (e.g., credit card fraud).\n",
    "Medical Diagnosis (e.g., disease prediction).\n",
    "Search Engine Ranking (e.g., Googleâ€™s ranking algorithms).\n",
    "Recommendation Systems (e.g., Netflix, Amazon).\n",
    "Financial Market Prediction (e.g., stock price forecasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb61f56-d5cf-4a28-905b-00075107a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.How does regularization help in XGBoost\f",
    "\n",
    "\u001d",
    "\u001d",
    "\n",
    "Regularization prevents overfitting by adding penalties:\n",
    "\n",
    "L1 Regularization (Lasso): Shrinks feature weights, promoting sparsity.\n",
    "L2 Regularization (Ridge): Prevents extreme weight values, making the model more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b4392-c18a-407f-8d9f-ac94aa725ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.What are some hyperparameters to tune in Gradient Boosting models\f",
    "\n",
    "\u001d",
    "\u0019Learning rate (eta): Controls step size in optimization.\n",
    "Number of trees: More trees improve accuracy but increase computation.\n",
    "Max depth: Higher values increase complexity but may cause overfitting.\n",
    "Subsample ratio: Controls how much data is used per tree.\n",
    "Regularization parameters (lambda, alpha): Help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55497e38-e2b0-43cf-91f1-aeb58f197c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.What is the concept of Feature Importance in Boosting\f",
    "\n",
    "\u001d",
    "\u001bFeature Importance indicates how much a feature contributes to predictions. It can be measured using:\n",
    "\n",
    "Gain: Contribution of a feature to model improvement.\n",
    "Cover: Number of samples affected by a feature.\n",
    "Frequency: How often a feature is used in trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c23c6-55fc-4c30-802f-133af3cca3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.Why is CatBoost efficient for categorical data?\n",
    "CatBoost is efficient because:\n",
    "\n",
    "It encodes categorical features internally, avoiding manual one-hot encoding.\n",
    "It uses Ordered Boosting, preventing target leakage.\n",
    "It applies efficient GPU acceleration, speeding up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71466fcc-9c2b-4d6f-a183-4aca27b91135",
   "metadata": {},
   "outputs": [],
   "source": [
    "##practical_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378243d2-2034-42ee-a193-3d7ae48978b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. Train an AdaBoost Classifier on a sample dataset and print model accuracy.\n",
    "\u001d",
    "\u0007from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train AdaBoost Classifier\n",
    "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc823e93-4b5d-4b0a-b67e-a8a606b9d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE).\n",
    "\u001d",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a sample regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train AdaBoost Regressor\n",
    "model = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"AdaBoost Regressor MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87438e-905a-4fb8-aec6-70888d4c88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance.\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train model\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print feature importance\n",
    "importances = model.feature_importances_\n",
    "for feature, importance in zip(data.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c5c51-3248-4456-bb05-ff9fed5610af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u001d",
    "#17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score.\n",
    "\u001d",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Train model\n",
    "regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Gradient Boosting Regressor R-Squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac5614-8b0b-41ea-9e4f-b10ddb5b1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting.\n",
    "\u001d",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train XGBoost Classifier\n",
    "xgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "# Compare with Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(\"XGBoost Classifier Accuracy:\", xgb_accuracy)\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", gb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ada9c-c02d-4563-98ec-f9fb561bce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19. Train a CatBoost Classifier and evaluate using F1-Score.\n",
    "\u0019\u000efrom catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Train CatBoost Classifier\n",
    "cat_model = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, verbose=False, random_state=42)\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = cat_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"CatBoost Classifier F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bdccf9-b94d-49db-9e91-e33c38e07a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE).\n",
    "\u0019from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train model\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"XGBoost Regressor MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a818e41-097e-4f24-ab70-16b023b7b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21. Train an AdaBoost Classifier and visualize feature importance.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, classification_report, log_loss\n",
    "from xgboost import XGBClassifier, XGBRegressor, plot_importance\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and visualize AdaBoost feature importance\n",
    "adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "plt.bar(range(X.shape[1]), adaboost.feature_importances_)\n",
    "plt.title(\"AdaBoost Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab1195-fca9-49c0-b99c-c13b7ccadb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u0019\u0019#22. Train a Gradient Boosting Regressor and plot learning curves.\n",
    "# Train Gradient Boosting Regressor and plot learning curve\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_reg.fit(X_train_reg, y_train_reg)\n",
    "plt.plot(gb_reg.train_score_, label=\"Training loss\")\n",
    "plt.title(\"Gradient Boosting Learning Curve\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dd3b5-7416-4a7c-a85b-21de635c0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u0019\u001b#23. Train an XGBoost Classifier and visualize feature importance.\n",
    "\u0019# Train XGBoost Classifier and visualize feature importance\n",
    "xgb = XGBClassifier(n_estimators=50, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "plot_importance(xgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd83e30-a53f-426c-8ba9-6232fa92fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24. Train a CatBoost Classifier and plot the confusion matrix.\n",
    "# Train CatBoost Classifier and plot confusion matrix\n",
    "catboost = CatBoostClassifier(n_estimators=50, verbose=0, random_state=42)\n",
    "catboost.fit(X_train, y_train)\n",
    "y_pred = catboost.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"CatBoost Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a149e5-7bd3-4ca5-8971-3a1dfa41e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u0019\u0007#25.Train an AdaBoost Classifier with different numbers of estimators and compare accuracy.\n",
    "# Train AdaBoost Classifier with different estimators and compare accuracy\n",
    "n_estimators = [10, 50, 100, 200]\n",
    "for n in n_estimators:\n",
    "    clf = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print(f\"AdaBoost with {n} estimators: Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fcea0-841c-4ffc-a60a-8ddb086b5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u0019\u0005#26. Train a Gradient Boosting Classifier and visualize the ROC curve.\n",
    "# Train Gradient Boosting Classifier and visualize the ROC curve\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_scores = gb_clf.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.4f}\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient Boosting ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c6026-d84d-48f0-b53e-bb316393dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u0019#27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV.\n",
    "\u0019# Train XGBoost Regressor and tune learning rate using GridSearchCV\n",
    "param_grid = {\"learning_rate\": [0.01, 0.1, 0.2, 0.3]}\n",
    "grid = GridSearchCV(XGBRegressor(n_estimators=100, random_state=42), param_grid, cv=3)\n",
    "grid.fit(X_train_reg, y_train_reg)\n",
    "print(f\"Best learning rate for XGBoost: {grid.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a31097-6f3b-49ea-8257-1bd711695e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\n",
    "\u0019\u0018# Train CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
    "X_imb, y_imb = make_classification(n_samples=1000, weights=[0.9, 0.1], n_features=20, random_state=42)\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(X_imb, y_imb, test_size=0.2, random_state=42)\n",
    "catboost_weighted = CatBoostClassifier(class_weights=[1, 10], verbose=0, random_state=42)\n",
    "catboost_weighted.fit(X_train_imb, y_train_imb)\n",
    "print(\"CatBoost Classification Report (Weighted):\")\n",
    "print(classification_report(y_test_imb, catboost_weighted.predict(X_test_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd4e76-1af1-4a94-8875-8e3ef7887175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#29. Train an AdaBoost Classifier and analyze the effect of different learning rates.\n",
    "\u001b\u000eimport numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define different learning rates to test\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "# Store results\n",
    "accuracies = []\n",
    "\n",
    "# Train AdaBoost models with different learning rates\n",
    "for lr in learning_rates:\n",
    "    model = AdaBoostClassifier(n_estimators=50, learning_rate=lr, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Learning Rate: {lr:.3f} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Plot learning rate vs. accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(learning_rates, accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of Learning Rate on AdaBoost Accuracy\")\n",
    "plt.xscale(\"log\")  # Log scale for better visualization\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3332e8-969a-4ac9-b670-a5f835b81322",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+001B (3929306178.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b\u000eimport numpy as np\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+001B\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53bb822-0424-4a9e-bad3-c64044365a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
    "# Train an XGBoost Classifier for multi-class classification and evaluate using log-loss\n",
    "X_multi, y_multi = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=15, random_state=42)\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
    "xgb_multi = XGBClassifier(n_estimators=50, objective=\"multi:softprob\", eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_multi.fit(X_train_multi, y_train_multi)\n",
    "y_pred_proba = xgb_multi.predict_proba(X_test_multi)\n",
    "print(f\"XGBoost Multi-class Log Loss: {log_loss(y_test_multi, y_pred_proba):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
